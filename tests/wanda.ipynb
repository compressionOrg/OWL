{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wanda实践\n",
    "\n",
    "本节将带大家一起揭开Wanda算法的神秘面纱～"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chenyuli/miniconda3/envs/owl/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llm(model_name, cache_dir=\"llm_weights\"):\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name, \n",
    "        torch_dtype=torch.float16, \n",
    "        cache_dir=cache_dir, \n",
    "        low_cpu_mem_usage=True, \n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "\n",
    "    model.seqlen = model.config.max_position_embeddings \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading llm model Enoch/llama-7b-hf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chenyuli/miniconda3/envs/owl/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 33/33 [00:07<00:00,  4.57it/s]\n",
      "/home/chenyuli/miniconda3/envs/owl/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model类型支持llama和opt类型\n",
    "model_name = \"Enoch/llama-7b-hf\"\n",
    "# model_name = \"facebook/opt-125m\"\n",
    "cache_dir = \"../llm_weights\"\n",
    "print(f\"loading llm model {model_name}\")\n",
    "model = get_llm(model_name, cache_dir)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_layers(module, layers=[nn.Linear], name=''):\n",
    "    \"\"\"\n",
    "    Recursively find the layers of a certain type in a module.\n",
    "\n",
    "    Args:\n",
    "        module (nn.Module): PyTorch module.\n",
    "        layers (list): List of layer types to find.\n",
    "        name (str): Name of the module.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary of layers of the given type(s) within the module.\n",
    "    \"\"\"\n",
    "    if type(module) in layers:\n",
    "        return {name: module}\n",
    "    res = {}\n",
    "    for name1, child in module.named_children():\n",
    "        res.update(find_layers(\n",
    "            child, layers=layers, name=name + '.' + name1 if name != '' else name1\n",
    "        ))\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaModel(\n",
       "  (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
       "  (layers): ModuleList(\n",
       "    (0-31): 32 x LlamaDecoderLayer(\n",
       "      (self_attn): LlamaSdpaAttention(\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (mlp): LlamaMLP(\n",
       "        (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "        (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "        (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "        (act_fn): SiLU()\n",
       "      )\n",
       "      (input_layernorm): LlamaRMSNorm()\n",
       "      (post_attention_layernorm): LlamaRMSNorm()\n",
       "    )\n",
       "  )\n",
       "  (norm): LlamaRMSNorm()\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_sparsity(model):\n",
    "    use_cache = model.config.use_cache \n",
    "    model.config.use_cache = False \n",
    "\n",
    "    if \"llama\" in model_name:\n",
    "        layers = model.model.layers\n",
    "    elif \"opt\" in model_name:\n",
    "        layers = model.model.decoder.layers\n",
    "    count = 0 \n",
    "    total_params = 0\n",
    "    for i in range(len(layers)):\n",
    "        layer = layers[i]\n",
    "        subset = find_layers(layer)\n",
    "\n",
    "        sub_count = 0\n",
    "        sub_params = 0\n",
    "        for name in subset:\n",
    "            W = subset[name].weight.data\n",
    "            count += (W==0).sum().item()\n",
    "            total_params += W.numel()\n",
    "\n",
    "            sub_count += (W==0).sum().item()\n",
    "            sub_params += W.numel()\n",
    "\n",
    "        print(f\"layer {i} sparsity {float(sub_count)/sub_params:.6f}\")\n",
    "\n",
    "    model.config.use_cache = use_cache \n",
    "    return float(count)/total_params "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 0 sparsity 0.000001\n",
      "layer 1 sparsity 0.000001\n",
      "layer 2 sparsity 0.000001\n",
      "layer 3 sparsity 0.000001\n",
      "layer 4 sparsity 0.000001\n",
      "layer 5 sparsity 0.000001\n",
      "layer 6 sparsity 0.000001\n",
      "layer 7 sparsity 0.000001\n",
      "layer 8 sparsity 0.000001\n",
      "layer 9 sparsity 0.000001\n",
      "layer 10 sparsity 0.000001\n",
      "layer 11 sparsity 0.000001\n",
      "layer 12 sparsity 0.000001\n",
      "layer 13 sparsity 0.000001\n",
      "layer 14 sparsity 0.000001\n",
      "layer 15 sparsity 0.000001\n",
      "layer 16 sparsity 0.000001\n",
      "layer 17 sparsity 0.000001\n",
      "layer 18 sparsity 0.000001\n",
      "layer 19 sparsity 0.000001\n",
      "layer 20 sparsity 0.000001\n",
      "layer 21 sparsity 0.000001\n",
      "layer 22 sparsity 0.000001\n",
      "layer 23 sparsity 0.000001\n",
      "layer 24 sparsity 0.000001\n",
      "layer 25 sparsity 0.000001\n",
      "layer 26 sparsity 0.000001\n",
      "layer 27 sparsity 0.000001\n",
      "layer 28 sparsity 0.000001\n",
      "layer 29 sparsity 0.000001\n",
      "layer 30 sparsity 0.000001\n",
      "layer 31 sparsity 0.000001\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0977446106431398e-06"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_sparsity(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wikitext2(nsamples, seed, seqlen, tokenizer):\n",
    "    # Load train and test datasets\n",
    "    traindata = load_dataset('wikitext', 'wikitext-2-raw-v1', split='train')\n",
    "    testdata = load_dataset('wikitext', 'wikitext-2-raw-v1', split='test')\n",
    "\n",
    "    # Encode datasets\n",
    "    trainenc = tokenizer(\" \".join(traindata['text']), return_tensors='pt')\n",
    "    testenc = tokenizer(\"\\n\\n\".join(testdata['text']), return_tensors='pt')\n",
    "\n",
    "    # Generate samples from training set\n",
    "    random.seed(seed)\n",
    "    trainloader = []\n",
    "    for _ in range(nsamples):\n",
    "        i = random.randint(0, trainenc.input_ids.shape[1] - seqlen - 1)\n",
    "        j = i + seqlen\n",
    "        inp = trainenc.input_ids[:, i:j]\n",
    "        tar = inp.clone()\n",
    "        tar[:, :-1] = -100\n",
    "        trainloader.append((inp, tar))\n",
    "    return trainloader, testenc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型的评价指标-困惑度PPL\n",
    "模型的困惑度是衡量语言模型性能的指标之一，通常用于评估模型在给定数据集上的预测能力。在自然语言处理中，困惑度是指模型对给定序列中下一个词的预测的困惑程度或不确定性程度。困惑度越低，表示模型在预测下一个词时越准确，即模型对数据集的预测更加自信。通常情况下，困惑度是一个正数，值越低表示模型性能越好。因此，困惑度可以作为评估语言模型质量和性能的重要指标之一。对于语句$s=w_1, w_2, w_3, \\ldots, w_n，其困惑度PPL可表示为：\n",
    "$\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\text { PPL }=p(s)^{-\\frac{1}{n}} \\\\\n",
    "& =p\\left(w_1, w_2, \\ldots, w_n\\right)^{\\frac{1}{n}} \\\\\n",
    "& =\\sqrt[n]{\\frac{1}{p\\left(w_1, w_2, \\ldots, w_n\\right)}} \\\\\n",
    "& =\\sqrt[n]{\\prod_{i=1}^n \\frac{1}{p\\left(w_i \\mid w_1, w_2, \\ldots, w_{i-1}\\right)}} \\\\\n",
    "&\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_ppl_wikitext(model, testenc, bs=1, device=None):\n",
    "    # Get input IDs\n",
    "    testenc = testenc.input_ids\n",
    "\n",
    "    # Calculate number of samples\n",
    "    nsamples = testenc.numel() // model.seqlen\n",
    "\n",
    "    # List to store negative log likelihoods\n",
    "    nlls = []\n",
    "    print(f\"nsamples {nsamples}\")\n",
    "\n",
    "    # Loop through each batch\n",
    "    for i in range(0,nsamples,bs):\n",
    "        if i % 50 == 0:\n",
    "            print(f\"sample {i}\")\n",
    "        # Calculate end index\n",
    "        j = min(i+bs, nsamples)\n",
    "\n",
    "        # Prepare inputs and move to device\n",
    "        inputs = testenc[:,(i * model.seqlen):(j * model.seqlen)].to(device)\n",
    "        inputs = inputs.reshape(j-i, model.seqlen)\n",
    "\n",
    "        # Forward pass through the model\n",
    "        lm_logits = model(inputs).logits\n",
    "\n",
    "        # Shift logits and labels for next token prediction\n",
    "        shift_logits = lm_logits[:, :-1, :].contiguous()\n",
    "        shift_labels = inputs[:, 1:]\n",
    "\n",
    "        # Compute loss\n",
    "        loss_fct = nn.CrossEntropyLoss()\n",
    "        loss = loss_fct(shift_logits.reshape(-1, shift_logits.size(-1)), shift_labels.reshape(-1))\n",
    "        print(f\"loss {loss}\")\n",
    "        # Calculate negative log likelihood\n",
    "        neg_log_likelihood = loss.float() * model.seqlen * (j-i)\n",
    "\n",
    "        # Append to list of negative log likelihoods\n",
    "        nlls.append(neg_log_likelihood)\n",
    "\n",
    "    # Compute perplexity\n",
    "    ppl = torch.exp(torch.stack(nlls).sum() / (nsamples * model.seqlen))\n",
    "\n",
    "    # Empty CUDA cache to save memory\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return ppl.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_ppl(model, tokenizer, device=torch.device(\"cuda:0\")):\n",
    "    # Print status\n",
    "    print(f\"evaluating on wikitext2\")\n",
    "\n",
    "    # Get the test loader\n",
    "    _, testloader = get_wikitext2(nsamples=128, seed=0, seqlen=model.seqlen, tokenizer=tokenizer )\n",
    "\n",
    "    # Evaluate ppl in no grad context to avoid updating the model\n",
    "    with torch.no_grad():\n",
    "        ppl_test = eval_ppl_wikitext(model, testloader, 1, device)\n",
    "    return ppl_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "# init tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_calibration_input(model, dataloader, device):\n",
    "    # 保存模型的原始缓存配置\n",
    "    use_cache = model.config.use_cache\n",
    "    # 禁用模型的缓存功能，确保每次输入都会被模型重新处理\n",
    "    model.config.use_cache = False\n",
    "    # 获取模型的所有层\n",
    "    if \"llama\" in model_name:\n",
    "        layers = model.model.layers\n",
    "    elif \"opt\" in model_name:\n",
    "        layers = model.model.decoder.layers\n",
    "\n",
    "    # 获取模型参数的数据类型\n",
    "    dtype = next(iter(model.parameters())).dtype\n",
    "    # 创建一个指定形状和数据类型的零张量，用来存储输入\n",
    "    inps = torch.zeros((128, model.seqlen, model.config.hidden_size), dtype=dtype, device=device)\n",
    "    # 设置不需要计算梯度，因为这里只是为了校准模型\n",
    "    inps.requires_grad = False\n",
    "    # 初始化一个缓存字典，用于存储处理过程中的信息\n",
    "    cache = {'i': 0, 'attention_mask': None, \"position_ids\": None}\n",
    "\n",
    "    # 定义一个内部类，用于捕获模型第一层的输入\n",
    "    class Catcher(nn.Module):\n",
    "        def __init__(self, module):\n",
    "            super().__init__()\n",
    "            self.module = module\n",
    "        def forward(self, inp, **kwargs):\n",
    "            # 存储输入到inps张量中，并更新缓存信息\n",
    "            inps[cache['i']] = inp\n",
    "            cache['i'] += 1\n",
    "            cache['attention_mask'] = kwargs['attention_mask']\n",
    "            if \"llama\" in model_name:\n",
    "                cache['position_ids'] = kwargs['position_ids']\n",
    "            # 抛出异常以中断前向传播\n",
    "            raise ValueError\n",
    "    # 将模型的第一层替换为Catcher类实例\n",
    "    layers[0] = Catcher(layers[0])\n",
    "    # 遍历数据加载器中的数据批次，并处理\n",
    "    for batch in dataloader:\n",
    "        try:\n",
    "            # 将数据批次送入模型进行处理，由于Catcher的存在会引发异常\n",
    "            model(batch[0].to(device))\n",
    "        except ValueError:\n",
    "            # 捕获异常，但不进行任何操作，目的是为了执行Catcher中的代码\n",
    "            pass \n",
    "    # 恢复模型的第一层为原来的层\n",
    "    layers[0] = layers[0].module\n",
    "\n",
    "    # 创建一个与输入张量形状和类型相同的零张量，用于存储输出\n",
    "    outs = torch.zeros_like(inps)\n",
    "    # 从缓存中取出attention_mask和position_ids\n",
    "    attention_mask = cache['attention_mask']\n",
    "    position_ids = cache['position_ids']\n",
    "    # 恢复模型的缓存设置\n",
    "    model.config.use_cache = use_cache\n",
    "    \n",
    "    if \"llama\" in model_name:\n",
    "        position_ids = cache['position_ids']\n",
    "        return inps, outs, attention_mask, position_ids \n",
    "    elif \"opt\" in model_name:\n",
    "        return inps, outs, attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrappedGPT:\n",
    "    \"\"\"\n",
    "    这个类封装了一个GPT层,用于特定的操作。\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, layer, layer_id=0, layer_name=\"none\"):\n",
    "        # 存储传入的层\n",
    "        self.layer = layer\n",
    "        # 从层的权重中获取设备信息\n",
    "        self.dev = self.layer.weight.device\n",
    "        # 获取权重的行数（输出维度大小）\n",
    "        self.rows = layer.weight.data.shape[0]\n",
    "        # 获取权重的列数（输入维度大小）\n",
    "        self.columns = layer.weight.data.shape[1]\n",
    "\n",
    "        # 初始化一个用于存储每列的缩放因子的向量，大小与权重的列数相同\n",
    "        self.scaler_row = torch.zeros((self.columns), device=self.dev)\n",
    "        # 初始化样本数量为0\n",
    "        self.nsamples = 0\n",
    "\n",
    "        # 存储层的ID和名称，这可能用于区分和跟踪不同的层\n",
    "        self.layer_id = layer_id \n",
    "        self.layer_name = layer_name\n",
    "\n",
    "    def add_batch(self, inp, out):\n",
    "        # 如果输入是二维的，添加一个维度使其成为三维的\n",
    "        if len(inp.shape) == 2:\n",
    "            inp = inp.unsqueeze(0)\n",
    "        # 获取批次的大小\n",
    "        tmp = inp.shape[0]\n",
    "        # 如果层是线性层，检查输入的维度，并可能将其重塑\n",
    "        if isinstance(self.layer, nn.Linear):\n",
    "            if len(inp.shape) == 3:\n",
    "                inp = inp.reshape((-1, inp.shape[-1]))\n",
    "            # 转置输入，因为PyTorch中的nn.Linear期望批次在第二维\n",
    "            inp = inp.t()\n",
    "\n",
    "        # 更新scaler_row向量，考虑到新加入的批次\n",
    "        self.scaler_row *= self.nsamples / (self.nsamples+tmp)\n",
    "        # 更新样本数量\n",
    "        self.nsamples += tmp\n",
    "\n",
    "        # 将输入转为float32类型\n",
    "        inp = inp.type(torch.float32)\n",
    "        # 更新scaler_row，根据新的输入调整每一列的缩放因子\n",
    "        self.scaler_row += torch.norm(inp, p=2, dim=1) ** 2  / self.nsamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_wanda_zscores(args, model, tokenizer, device=torch.device(\"cuda:0\"), prune_n=0, prune_m=0):\n",
    "    ##### calucalte outlier ratio\n",
    "    \n",
    "    \n",
    "    \n",
    "    all_layer_ratio=[]\n",
    "    use_cache = model.config.use_cache \n",
    "    model.config.use_cache = False \n",
    "\n",
    "    print(\"loading calibdation data\")\n",
    "    dataloader, _ = get_loaders(\"c4\",nsamples=args.nsamples,seed=args.seed,seqlen=2048,tokenizer=tokenizer)\n",
    "    print(\"dataset loading complete\")\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        if \"OPT\" in model.__class__.__name__:\n",
    "            \n",
    "            inps, outs, attention_mask, position_ids = prepare_calibration_input_opt(model, dataloader, device)\n",
    "        else:\n",
    "            \n",
    "            inps, outs, attention_mask, position_ids = prepare_calibration_input(model, dataloader, device)\n",
    "\n",
    "\n",
    "\n",
    "    print (\"inps\",inps)\n",
    "    if \"opt\" in args.model:\n",
    "        layers=model.model.decoder.layers\n",
    "        \n",
    "    else:\n",
    "        layers = model.model.layers\n",
    "\n",
    "\n",
    "    for i in range(len(layers)):\n",
    "        layer = layers[i]\n",
    "\n",
    "        subset = find_layers(layer)\n",
    "\n",
    "        if f\"model.layers.{i}\" in model.hf_device_map:   ## handle the case for llama-30B and llama-65B, when the device map has multiple GPUs;\n",
    "            dev = model.hf_device_map[f\"model.layers.{i}\"]\n",
    "            # inps, outs,  position_ids = inps.to(dev), outs.to(dev),  position_ids.to(dev)\n",
    "            inps, outs, position_ids = inps.to(dev), outs.to(dev), position_ids.to(dev)\n",
    "\n",
    "        wrapped_layers = {}\n",
    "        for name in subset:\n",
    "            wrapped_layers[name] = WrappedGPT(subset[name])\n",
    "\n",
    "        def add_batch(name):\n",
    "            def tmp(_, inp, out):\n",
    "                wrapped_layers[name].add_batch(inp[0].data, out.data)\n",
    "            return tmp\n",
    "\n",
    "        handles = []\n",
    "        for name in wrapped_layers:\n",
    "            handles.append(subset[name].register_forward_hook(add_batch(name)))\n",
    "        for j in range(args.nsamples):\n",
    "            with torch.no_grad():\n",
    "                if \"OPT\" in model.__class__.__name__:\n",
    "                    outs[j] = layer(inps[j].unsqueeze(0), attention_mask=attention_mask)[0]\n",
    "                else:\n",
    "                    outs[j] = layer(inps[j].unsqueeze(0), attention_mask=attention_mask, position_ids=position_ids)[0]\n",
    "        for h in handles:\n",
    "            h.remove()\n",
    "            \n",
    "            \n",
    "        layer_wmetric=[]\n",
    "        layer_ametric=[]\n",
    "\n",
    "        for name in subset:\n",
    "            \n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "            print(f\"pruning layer {i} name {name}\")\n",
    "            W_metric = torch.abs(subset[name].weight.data) * torch.sqrt(wrapped_layers[name].scaler_row.reshape((1,-1)))\n",
    "\n",
    "\n",
    "            activation_data=torch.sqrt(wrapped_layers[name].scaler_row.reshape((1,-1)))\n",
    "            layer_wmetric.append(W_metric)   \n",
    "            layer_ametric.append(activation_data) \n",
    "                \n",
    "\n",
    "        for j in range(args.nsamples):\n",
    "            with torch.no_grad():\n",
    "                if \"OPT\" in model.__class__.__name__:\n",
    "                    outs[j] = layer(inps[j].unsqueeze(0), attention_mask=attention_mask)[0]\n",
    "                else:\n",
    "                    outs[j] = layer(inps[j].unsqueeze(0), attention_mask=attention_mask, position_ids=position_ids)[0]\n",
    "        inps, outs = outs, inps\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        layer_wmetric = torch.cat([torch.flatten(x.cpu()) for x in layer_wmetric])\n",
    "        layer_ametic = torch.cat([torch.flatten(x.cpu()) for x in layer_ametric])\n",
    "        # OWL\n",
    "        # for out_ratio in [args.Hyper_m]:\n",
    "            \n",
    "        #     out_ratio_layer=check_outlier_mean(layer_wmetric,out_ratio)\n",
    "        #     print (\"layer outlier ratio\",out_ratio,out_ratio_layer)\n",
    "            \n",
    "        # TODO:\n",
    "        # z_scores\n",
    "        out_ratio_layer = get_z_scores_sum(layer_ametic)\n",
    "        print (\"layer zscores ratio\",out_ratio_layer)\n",
    "        all_layer_ratio.append(out_ratio_layer)\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    print (\"before adjustment\",all_layer_ratio)\n",
    "\n",
    "    \n",
    " \n",
    "    \n",
    "\n",
    "    all_layer_ratio=np.array(all_layer_ratio)\n",
    "    # OWL\n",
    "    # all_layer_ratio = ((all_layer_ratio - all_layer_ratio.min()) * (1/(all_layer_ratio.max() - all_layer_ratio.min()) * args.Lamda*2))\n",
    "    \n",
    "    # max-min normalization\n",
    "    # all_layer_ratio = ((all_layer_ratio - all_layer_ratio.min()) * (1/(all_layer_ratio.max() - all_layer_ratio.min())))\n",
    "    all_layer_ratio = ((all_layer_ratio - all_layer_ratio.min()) * (1/(all_layer_ratio.max() - all_layer_ratio.min()) * args.Lamda*2))\n",
    "    \n",
    "    all_layer_ratio=all_layer_ratio-np.mean(all_layer_ratio)+(1-args.sparsity_ratio)\n",
    "   \n",
    "    print (all_layer_ratio,np.mean(all_layer_ratio),np.max(all_layer_ratio),np.min(all_layer_ratio))\n",
    "    # st()\n",
    "   \n",
    "    \n",
    "                \n",
    "        \n",
    "    \n",
    "    print (\"after adjustment\",all_layer_ratio  )\n",
    "    \n",
    "\n",
    "\n",
    "    model.config.use_cache = use_cache \n",
    "    torch.cuda.empty_cache()\n",
    "    ############## prune\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    use_cache = model.config.use_cache \n",
    "    model.config.use_cache = False \n",
    "\n",
    "    print(\"loading calibdation data\")\n",
    "    dataloader, _ = get_loaders(\"c4\",nsamples=args.nsamples,seed=args.seed,seqlen=2048,tokenizer=tokenizer)\n",
    "    print(\"dataset loading complete\")\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        if \"OPT\" in model.__class__.__name__:\n",
    "            \n",
    "            inps, outs, attention_mask, position_ids = prepare_calibration_input_opt(model, dataloader, device)\n",
    "        else:\n",
    "            \n",
    "            inps, outs, attention_mask, position_ids = prepare_calibration_input(model, dataloader, device)\n",
    "\n",
    "\n",
    "\n",
    "    print (\"inps\",inps)\n",
    "    if \"opt\" in args.model:\n",
    "        layers=model.model.decoder.layers\n",
    "        \n",
    "    else:\n",
    "        layers = model.model.layers\n",
    "\n",
    "\n",
    "    for i in range(len(layers)):\n",
    "        layer = layers[i]\n",
    "\n",
    "        subset = find_layers(layer)\n",
    "\n",
    "        if f\"model.layers.{i}\" in model.hf_device_map:   ## handle the case for llama-30B and llama-65B, when the device map has multiple GPUs;\n",
    "            dev = model.hf_device_map[f\"model.layers.{i}\"]\n",
    "            # inps, outs,  position_ids = inps.to(dev), outs.to(dev),  position_ids.to(dev)\n",
    "            inps, outs,  position_ids = inps.to(dev), outs.to(dev),  position_ids.to(dev)\n",
    "\n",
    "        wrapped_layers = {}\n",
    "        for name in subset:\n",
    "            wrapped_layers[name] = WrappedGPT(subset[name])\n",
    "\n",
    "        def add_batch(name):\n",
    "            def tmp(_, inp, out):\n",
    "                wrapped_layers[name].add_batch(inp[0].data, out.data)\n",
    "            return tmp\n",
    "\n",
    "        handles = []\n",
    "        for name in wrapped_layers:\n",
    "            handles.append(subset[name].register_forward_hook(add_batch(name)))\n",
    "        for j in range(args.nsamples):\n",
    "            with torch.no_grad():\n",
    "                if \"OPT\" in model.__class__.__name__:\n",
    "                    outs[j] = layer(inps[j].unsqueeze(0), attention_mask=attention_mask)[0]\n",
    "                else:\n",
    "                    outs[j] = layer(inps[j].unsqueeze(0), attention_mask=attention_mask, position_ids=position_ids)[0]\n",
    "        for h in handles:\n",
    "            h.remove()\n",
    "            \n",
    "            \n",
    "        \n",
    "\n",
    "        for name in subset:\n",
    "            \n",
    "\n",
    "            print(f\"pruning layer {i} name {name}\")\n",
    "            W_metric = torch.abs(subset[name].weight.data) * torch.sqrt(wrapped_layers[name].scaler_row.reshape((1,-1)))\n",
    "            \n",
    "            # TODO:add entropy\n",
    "            # pr = torch.abs(W_metric)/torch.sum(torch.abs(W_metric), dim=0)\n",
    "            # pc = torch.abs(W_metric)/torch.sum(torch.abs(W_metric), dim=1).reshape(-1, 1)\n",
    "            # W_metric = torch.abs((-pr * torch.log(pr)) - (pc * torch.log(pc)))\n",
    "            \n",
    "            # TODO:add wentropy\n",
    "            # # 定义一个很小的常数 epsilon 避免计算对数时的问题\n",
    "            # epsilon = 1e-10\n",
    "            # W_metric += epsilon\n",
    "            # # 归一化张量使其元素和为1\n",
    "            # probabilities = W_metric / W_metric.sum()\n",
    "            # # 计算概率分布的对数\n",
    "            # log_probabilities = torch.log(probabilities)\n",
    "            # # 为每个元素计算熵\n",
    "            # H = -probabilities * log_probabilities\n",
    "            # W_metric = torch.abs(W_metric * H)\n",
    "\n",
    "            \n",
    "            \n",
    "            activation_data=torch.sqrt(wrapped_layers[name].scaler_row.reshape((1,-1)))\n",
    "\n",
    "            layer_sparsity_ratio= 1-all_layer_ratio[i]\n",
    "            \n",
    "            \n",
    "            if layer_sparsity_ratio<=0:\n",
    "                layer_sparsity_ratio=0.01\n",
    "\n",
    "            W_mask = (torch.zeros_like(W_metric) == 1)  ## initialize a mask to be all False\n",
    "            if prune_n != 0:\n",
    "                # structured n:m sparsity\n",
    "                for ii in range(W_metric.shape[1]):\n",
    "                    if ii % prune_m == 0:\n",
    "                        tmp = W_metric[:,ii:(ii+prune_m)].float()\n",
    "                        W_mask.scatter_(1,ii+torch.topk(tmp, prune_n,dim=1, largest=False)[1], True)\n",
    "            else:\n",
    "                sort_res = torch.sort(W_metric, dim=-1, stable=True)\n",
    "\n",
    "                if args.use_variant:\n",
    "                    # wanda variant \n",
    "                    tmp_metric = torch.cumsum(sort_res[0], dim=1)\n",
    "                    sum_before = W_metric.sum(dim=1)\n",
    "\n",
    "                    alpha = 0.4\n",
    "                    alpha_hist = [0., 0.8]\n",
    "                    W_mask, cur_sparsity = return_given_alpha(alpha, sort_res, W_metric, tmp_metric, sum_before)\n",
    "                    while (torch.abs(cur_sparsity - layer_sparsity_ratio)>0.001) and (alpha_hist[1]-alpha_hist[0]>=0.001):\n",
    "                        if cur_sparsity > layer_sparsity_ratio:\n",
    "                            alpha_new = (alpha + alpha_hist[0]) / 2.0\n",
    "                            alpha_hist[1] = alpha\n",
    "                        else:\n",
    "                            alpha_new = (alpha + alpha_hist[1]) / 2.0\n",
    "                            alpha_hist[0] = alpha\n",
    "\n",
    "                        alpha = alpha_new \n",
    "                        W_mask, cur_sparsity = return_given_alpha(alpha, sort_res, W_metric, tmp_metric, sum_before)\n",
    "                    print(f\"alpha found {alpha} sparsity {cur_sparsity:.6f}\")\n",
    "                else:\n",
    "                    # unstructured pruning\n",
    "                    indices = sort_res[1][:,:int(W_metric.shape[1]*layer_sparsity_ratio)]\n",
    "                    W_mask.scatter_(1, indices, True)\n",
    "#             print (\"W_mask\",W_mask)\n",
    "            subset[name].weight.data[W_mask] = 0  ## set weights to zero \n",
    "\n",
    "        for j in range(args.nsamples):\n",
    "            with torch.no_grad():\n",
    "                if \"OPT\" in model.__class__.__name__:\n",
    "                    outs[j] = layer(inps[j].unsqueeze(0), attention_mask=attention_mask)[0]\n",
    "                else:\n",
    "                    outs[j] = layer(inps[j].unsqueeze(0), attention_mask=attention_mask, position_ids=position_ids)[0]\n",
    "        inps, outs = outs, inps\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    model.config.use_cache = use_cache \n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_wanda(model, tokenizer, device=torch.device(\"cuda:0\"),nsamples=128, seed=0, sparsity_ratio=0.2, prune_n=0, prune_m=0):\n",
    "    # 保存原始模型缓存配置，并暂时禁用它。\n",
    "    # 确保修剪校准期间不使用之前的计算结果。\n",
    "    use_cache = model.config.use_cache \n",
    "    model.config.use_cache = False \n",
    "\n",
    "    # 开始加载校准数据，并在加载完成后通知。\n",
    "    print(\"loading calibration data\")\n",
    "    dataloader, _ = get_wikitext2(\n",
    "        nsamples=nsamples, \n",
    "        seed=seed, \n",
    "        seqlen=model.seqlen, \n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "    print(\"dataset loading complete\")\n",
    "\n",
    "    # 准备校准输入，同时不追踪梯度以提高效率。\n",
    "    with torch.no_grad():\n",
    "        if \"llama\" in model_name:\n",
    "            inps, outs, attention_mask, position_ids = prepare_calibration_input(model, dataloader, device)\n",
    "        elif \"opt\" in model_name:\n",
    "            inps, outs, attention_mask = prepare_calibration_input(model, dataloader, device)\n",
    "\n",
    "    # 获取模型内部的层列表。\n",
    "    if \"llama\" in model_name:\n",
    "        layers = model.model.layers\n",
    "    elif \"opt\" in model_name:\n",
    "        layers = model.model.decoder.layers\n",
    "\n",
    "    # 遍历每一层进行修剪操作。\n",
    "    for i in range(len(layers)):\n",
    "        layer = layers[i]\n",
    "        subset = find_layers(layer)  # 查找需要修剪的层的子集。\n",
    "\n",
    "        # 初始化一个字典用于存储层的包装器。\n",
    "        wrapped_layers = {}\n",
    "        for name in subset:\n",
    "            wrapped_layers[name] = WrappedGPT(subset[name])\n",
    "\n",
    "        # 定义添加批处理数据的函数，用于钩子中。\n",
    "        def add_batch(name):\n",
    "            # 定义临时函数，获取输入输出并添加到对应的包装层。\n",
    "            def tmp(_, inp, out):\n",
    "                wrapped_layers[name].add_batch(inp[0].data, out.data)\n",
    "            return tmp\n",
    "\n",
    "        # 注册前向钩子，并将句柄添加到列表以便之后移除。\n",
    "        handles = []\n",
    "        for name in wrapped_layers:\n",
    "            handles.append(subset[name].register_forward_hook(add_batch(name)))\n",
    "\n",
    "        # 对每个校准样本执行前向传播，并收集数据。\n",
    "        for j in range(nsamples):\n",
    "            with torch.no_grad():\n",
    "                if \"llama\" in model_name:\n",
    "                    outs[j] = layer(inps[j].unsqueeze(0), attention_mask=attention_mask, position_ids=position_ids)[0]\n",
    "                elif \"opt\" in model_name:\n",
    "                    outs[j] = layer(inps[j].unsqueeze(0), attention_mask=attention_mask)[0]\n",
    "\n",
    "\n",
    "        # 移除之前注册的所有钩子。\n",
    "        for h in handles:\n",
    "            h.remove()\n",
    "\n",
    "        # 对每个子集中的层进行修剪操作。\n",
    "        for name in subset:\n",
    "            print(f\"pruning layer {i} name {name}\")\n",
    "            # 计算修剪度量，基于权重的绝对值和对应的激活函数\n",
    "            W_metric = torch.abs(subset[name].weight.data) * torch.sqrt(wrapped_layers[name].scaler_row.reshape((1,-1)))\n",
    "            # 初始化修剪掩码，开始时全为False。\n",
    "            W_mask = (torch.zeros_like(W_metric) == 1)\n",
    "\n",
    "            # 如果设置了结构化修剪参数，则执行结构化修剪。\n",
    "            if prune_n != 0:\n",
    "                # 结构化n:m稀疏性\n",
    "                for ii in range(W_metric.shape[1]):\n",
    "                    if ii % prune_m == 0:\n",
    "                        tmp = W_metric[:, ii:(ii+prune_m)].float()\n",
    "                        W_mask.scatter_(1, ii + torch.topk(tmp, prune_n, dim=1, largest=False)[1], True)\n",
    "            else:\n",
    "                # 非结构化修剪\n",
    "                sort_res = torch.sort(W_metric, dim=-1, stable=True)\n",
    "                indices = sort_res[1][:, :int(W_metric.shape[1] * sparsity_ratio)]\n",
    "                W_mask.scatter_(1, indices, True)\n",
    "\n",
    "            # 最后将掩码为True的权重值设为零，完成修剪。\n",
    "            subset[name].weight.data[W_mask] = 0\n",
    "\n",
    "        # 再次对每个样本执行前向传播，可能用于验证修剪效果。\n",
    "        for j in range(nsamples):\n",
    "            with torch.no_grad():\n",
    "                if \"llama\" in model_name:\n",
    "                    outs[j] = layer(inps[j].unsqueeze(0), attention_mask=attention_mask, position_ids=position_ids)[0]\n",
    "                elif \"opt\" in model_name:\n",
    "                    outs[j] = layer(inps[j].unsqueeze(0), attention_mask=attention_mask)[0]\n",
    "\n",
    "\n",
    "        # 交换输入和输出的引用，为下一轮或后续操作准备。\n",
    "        inps, outs = outs, inps\n",
    "\n",
    "    # 恢复模型的缓存设置。\n",
    "    model.config.use_cache = use_cache \n",
    "\n",
    "    # 清空CUDA缓存，以减少内存消耗。\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading calibration data\n",
      "dataset loading complete\n",
      "pruning layer 0 name self_attn.q_proj\n",
      "pruning layer 0 name self_attn.k_proj\n",
      "pruning layer 0 name self_attn.v_proj\n",
      "pruning layer 0 name self_attn.o_proj\n",
      "pruning layer 0 name mlp.gate_proj\n",
      "pruning layer 0 name mlp.down_proj\n",
      "pruning layer 0 name mlp.up_proj\n",
      "pruning layer 1 name self_attn.q_proj\n",
      "pruning layer 1 name self_attn.k_proj\n",
      "pruning layer 1 name self_attn.v_proj\n",
      "pruning layer 1 name self_attn.o_proj\n",
      "pruning layer 1 name mlp.gate_proj\n",
      "pruning layer 1 name mlp.down_proj\n",
      "pruning layer 1 name mlp.up_proj\n",
      "pruning layer 2 name self_attn.q_proj\n",
      "pruning layer 2 name self_attn.k_proj\n",
      "pruning layer 2 name self_attn.v_proj\n",
      "pruning layer 2 name self_attn.o_proj\n",
      "pruning layer 2 name mlp.gate_proj\n",
      "pruning layer 2 name mlp.down_proj\n",
      "pruning layer 2 name mlp.up_proj\n",
      "pruning layer 3 name self_attn.q_proj\n",
      "pruning layer 3 name self_attn.k_proj\n",
      "pruning layer 3 name self_attn.v_proj\n",
      "pruning layer 3 name self_attn.o_proj\n",
      "pruning layer 3 name mlp.gate_proj\n",
      "pruning layer 3 name mlp.down_proj\n",
      "pruning layer 3 name mlp.up_proj\n",
      "pruning layer 4 name self_attn.q_proj\n",
      "pruning layer 4 name self_attn.k_proj\n",
      "pruning layer 4 name self_attn.v_proj\n",
      "pruning layer 4 name self_attn.o_proj\n",
      "pruning layer 4 name mlp.gate_proj\n",
      "pruning layer 4 name mlp.down_proj\n",
      "pruning layer 4 name mlp.up_proj\n",
      "pruning layer 5 name self_attn.q_proj\n",
      "pruning layer 5 name self_attn.k_proj\n",
      "pruning layer 5 name self_attn.v_proj\n",
      "pruning layer 5 name self_attn.o_proj\n",
      "pruning layer 5 name mlp.gate_proj\n",
      "pruning layer 5 name mlp.down_proj\n",
      "pruning layer 5 name mlp.up_proj\n",
      "pruning layer 6 name self_attn.q_proj\n",
      "pruning layer 6 name self_attn.k_proj\n",
      "pruning layer 6 name self_attn.v_proj\n",
      "pruning layer 6 name self_attn.o_proj\n",
      "pruning layer 6 name mlp.gate_proj\n",
      "pruning layer 6 name mlp.down_proj\n",
      "pruning layer 6 name mlp.up_proj\n",
      "pruning layer 7 name self_attn.q_proj\n",
      "pruning layer 7 name self_attn.k_proj\n",
      "pruning layer 7 name self_attn.v_proj\n",
      "pruning layer 7 name self_attn.o_proj\n",
      "pruning layer 7 name mlp.gate_proj\n",
      "pruning layer 7 name mlp.down_proj\n",
      "pruning layer 7 name mlp.up_proj\n",
      "pruning layer 8 name self_attn.q_proj\n",
      "pruning layer 8 name self_attn.k_proj\n",
      "pruning layer 8 name self_attn.v_proj\n",
      "pruning layer 8 name self_attn.o_proj\n",
      "pruning layer 8 name mlp.gate_proj\n",
      "pruning layer 8 name mlp.down_proj\n",
      "pruning layer 8 name mlp.up_proj\n",
      "pruning layer 9 name self_attn.q_proj\n",
      "pruning layer 9 name self_attn.k_proj\n",
      "pruning layer 9 name self_attn.v_proj\n",
      "pruning layer 9 name self_attn.o_proj\n",
      "pruning layer 9 name mlp.gate_proj\n",
      "pruning layer 9 name mlp.down_proj\n",
      "pruning layer 9 name mlp.up_proj\n",
      "pruning layer 10 name self_attn.q_proj\n",
      "pruning layer 10 name self_attn.k_proj\n",
      "pruning layer 10 name self_attn.v_proj\n",
      "pruning layer 10 name self_attn.o_proj\n",
      "pruning layer 10 name mlp.gate_proj\n",
      "pruning layer 10 name mlp.down_proj\n",
      "pruning layer 10 name mlp.up_proj\n",
      "pruning layer 11 name self_attn.q_proj\n",
      "pruning layer 11 name self_attn.k_proj\n",
      "pruning layer 11 name self_attn.v_proj\n",
      "pruning layer 11 name self_attn.o_proj\n",
      "pruning layer 11 name mlp.gate_proj\n",
      "pruning layer 11 name mlp.down_proj\n",
      "pruning layer 11 name mlp.up_proj\n",
      "pruning layer 12 name self_attn.q_proj\n",
      "pruning layer 12 name self_attn.k_proj\n",
      "pruning layer 12 name self_attn.v_proj\n",
      "pruning layer 12 name self_attn.o_proj\n",
      "pruning layer 12 name mlp.gate_proj\n",
      "pruning layer 12 name mlp.down_proj\n",
      "pruning layer 12 name mlp.up_proj\n",
      "pruning layer 13 name self_attn.q_proj\n",
      "pruning layer 13 name self_attn.k_proj\n",
      "pruning layer 13 name self_attn.v_proj\n",
      "pruning layer 13 name self_attn.o_proj\n",
      "pruning layer 13 name mlp.gate_proj\n",
      "pruning layer 13 name mlp.down_proj\n",
      "pruning layer 13 name mlp.up_proj\n",
      "pruning layer 14 name self_attn.q_proj\n",
      "pruning layer 14 name self_attn.k_proj\n",
      "pruning layer 14 name self_attn.v_proj\n",
      "pruning layer 14 name self_attn.o_proj\n",
      "pruning layer 14 name mlp.gate_proj\n",
      "pruning layer 14 name mlp.down_proj\n",
      "pruning layer 14 name mlp.up_proj\n",
      "pruning layer 15 name self_attn.q_proj\n",
      "pruning layer 15 name self_attn.k_proj\n",
      "pruning layer 15 name self_attn.v_proj\n",
      "pruning layer 15 name self_attn.o_proj\n",
      "pruning layer 15 name mlp.gate_proj\n",
      "pruning layer 15 name mlp.down_proj\n",
      "pruning layer 15 name mlp.up_proj\n",
      "pruning layer 16 name self_attn.q_proj\n",
      "pruning layer 16 name self_attn.k_proj\n",
      "pruning layer 16 name self_attn.v_proj\n",
      "pruning layer 16 name self_attn.o_proj\n",
      "pruning layer 16 name mlp.gate_proj\n",
      "pruning layer 16 name mlp.down_proj\n",
      "pruning layer 16 name mlp.up_proj\n",
      "pruning layer 17 name self_attn.q_proj\n",
      "pruning layer 17 name self_attn.k_proj\n",
      "pruning layer 17 name self_attn.v_proj\n",
      "pruning layer 17 name self_attn.o_proj\n",
      "pruning layer 17 name mlp.gate_proj\n",
      "pruning layer 17 name mlp.down_proj\n",
      "pruning layer 17 name mlp.up_proj\n",
      "pruning layer 18 name self_attn.q_proj\n",
      "pruning layer 18 name self_attn.k_proj\n",
      "pruning layer 18 name self_attn.v_proj\n",
      "pruning layer 18 name self_attn.o_proj\n",
      "pruning layer 18 name mlp.gate_proj\n",
      "pruning layer 18 name mlp.down_proj\n",
      "pruning layer 18 name mlp.up_proj\n",
      "pruning layer 19 name self_attn.q_proj\n",
      "pruning layer 19 name self_attn.k_proj\n",
      "pruning layer 19 name self_attn.v_proj\n",
      "pruning layer 19 name self_attn.o_proj\n",
      "pruning layer 19 name mlp.gate_proj\n",
      "pruning layer 19 name mlp.down_proj\n",
      "pruning layer 19 name mlp.up_proj\n",
      "pruning layer 20 name self_attn.q_proj\n",
      "pruning layer 20 name self_attn.k_proj\n",
      "pruning layer 20 name self_attn.v_proj\n",
      "pruning layer 20 name self_attn.o_proj\n",
      "pruning layer 20 name mlp.gate_proj\n",
      "pruning layer 20 name mlp.down_proj\n",
      "pruning layer 20 name mlp.up_proj\n",
      "pruning layer 21 name self_attn.q_proj\n",
      "pruning layer 21 name self_attn.k_proj\n",
      "pruning layer 21 name self_attn.v_proj\n",
      "pruning layer 21 name self_attn.o_proj\n",
      "pruning layer 21 name mlp.gate_proj\n",
      "pruning layer 21 name mlp.down_proj\n",
      "pruning layer 21 name mlp.up_proj\n",
      "pruning layer 22 name self_attn.q_proj\n",
      "pruning layer 22 name self_attn.k_proj\n",
      "pruning layer 22 name self_attn.v_proj\n",
      "pruning layer 22 name self_attn.o_proj\n",
      "pruning layer 22 name mlp.gate_proj\n",
      "pruning layer 22 name mlp.down_proj\n",
      "pruning layer 22 name mlp.up_proj\n",
      "pruning layer 23 name self_attn.q_proj\n",
      "pruning layer 23 name self_attn.k_proj\n",
      "pruning layer 23 name self_attn.v_proj\n",
      "pruning layer 23 name self_attn.o_proj\n",
      "pruning layer 23 name mlp.gate_proj\n",
      "pruning layer 23 name mlp.down_proj\n",
      "pruning layer 23 name mlp.up_proj\n",
      "pruning layer 24 name self_attn.q_proj\n",
      "pruning layer 24 name self_attn.k_proj\n",
      "pruning layer 24 name self_attn.v_proj\n",
      "pruning layer 24 name self_attn.o_proj\n",
      "pruning layer 24 name mlp.gate_proj\n",
      "pruning layer 24 name mlp.down_proj\n",
      "pruning layer 24 name mlp.up_proj\n",
      "pruning layer 25 name self_attn.q_proj\n",
      "pruning layer 25 name self_attn.k_proj\n",
      "pruning layer 25 name self_attn.v_proj\n",
      "pruning layer 25 name self_attn.o_proj\n",
      "pruning layer 25 name mlp.gate_proj\n",
      "pruning layer 25 name mlp.down_proj\n",
      "pruning layer 25 name mlp.up_proj\n",
      "pruning layer 26 name self_attn.q_proj\n",
      "pruning layer 26 name self_attn.k_proj\n",
      "pruning layer 26 name self_attn.v_proj\n",
      "pruning layer 26 name self_attn.o_proj\n",
      "pruning layer 26 name mlp.gate_proj\n",
      "pruning layer 26 name mlp.down_proj\n",
      "pruning layer 26 name mlp.up_proj\n",
      "pruning layer 27 name self_attn.q_proj\n",
      "pruning layer 27 name self_attn.k_proj\n",
      "pruning layer 27 name self_attn.v_proj\n",
      "pruning layer 27 name self_attn.o_proj\n",
      "pruning layer 27 name mlp.gate_proj\n",
      "pruning layer 27 name mlp.down_proj\n",
      "pruning layer 27 name mlp.up_proj\n",
      "pruning layer 28 name self_attn.q_proj\n",
      "pruning layer 28 name self_attn.k_proj\n",
      "pruning layer 28 name self_attn.v_proj\n",
      "pruning layer 28 name self_attn.o_proj\n",
      "pruning layer 28 name mlp.gate_proj\n",
      "pruning layer 28 name mlp.down_proj\n",
      "pruning layer 28 name mlp.up_proj\n",
      "pruning layer 29 name self_attn.q_proj\n",
      "pruning layer 29 name self_attn.k_proj\n",
      "pruning layer 29 name self_attn.v_proj\n",
      "pruning layer 29 name self_attn.o_proj\n",
      "pruning layer 29 name mlp.gate_proj\n",
      "pruning layer 29 name mlp.down_proj\n",
      "pruning layer 29 name mlp.up_proj\n",
      "pruning layer 30 name self_attn.q_proj\n",
      "pruning layer 30 name self_attn.k_proj\n",
      "pruning layer 30 name self_attn.v_proj\n",
      "pruning layer 30 name self_attn.o_proj\n",
      "pruning layer 30 name mlp.gate_proj\n",
      "pruning layer 30 name mlp.down_proj\n",
      "pruning layer 30 name mlp.up_proj\n",
      "pruning layer 31 name self_attn.q_proj\n",
      "pruning layer 31 name self_attn.k_proj\n",
      "pruning layer 31 name self_attn.v_proj\n",
      "pruning layer 31 name self_attn.o_proj\n",
      "pruning layer 31 name mlp.gate_proj\n",
      "pruning layer 31 name mlp.down_proj\n",
      "pruning layer 31 name mlp.up_proj\n"
     ]
    }
   ],
   "source": [
    "# prune model\n",
    "prune_wanda(model, tokenizer, sparsity_ratio=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model = \"wanda/{}\".format(model_name.split('/')[-1])\n",
    "# print(save_model)\n",
    "model.save_pretrained(save_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************\n",
      "layer 0 sparsity 0.299840\n",
      "layer 1 sparsity 0.299840\n",
      "layer 2 sparsity 0.299840\n",
      "layer 3 sparsity 0.299840\n",
      "layer 4 sparsity 0.299840\n",
      "layer 5 sparsity 0.299840\n",
      "layer 6 sparsity 0.299840\n",
      "layer 7 sparsity 0.299840\n",
      "layer 8 sparsity 0.299840\n",
      "layer 9 sparsity 0.299840\n",
      "layer 10 sparsity 0.299840\n",
      "layer 11 sparsity 0.299840\n",
      "layer 12 sparsity 0.299840\n",
      "layer 13 sparsity 0.299840\n",
      "layer 14 sparsity 0.299840\n",
      "layer 15 sparsity 0.299840\n",
      "layer 16 sparsity 0.299840\n",
      "layer 17 sparsity 0.299840\n",
      "layer 18 sparsity 0.299840\n",
      "layer 19 sparsity 0.299840\n",
      "layer 20 sparsity 0.299840\n",
      "layer 21 sparsity 0.299840\n",
      "layer 22 sparsity 0.299840\n",
      "layer 23 sparsity 0.299840\n",
      "layer 24 sparsity 0.299840\n",
      "layer 25 sparsity 0.299840\n",
      "layer 26 sparsity 0.299840\n",
      "layer 27 sparsity 0.299840\n",
      "layer 28 sparsity 0.299840\n",
      "layer 29 sparsity 0.299840\n",
      "layer 30 sparsity 0.299840\n",
      "layer 31 sparsity 0.299840\n",
      "pruned model sparsity sanity check 0.2998\n",
      "******************************\n"
     ]
    }
   ],
   "source": [
    "print(\"*\"*30)\n",
    "sparsity_ratio = check_sparsity(model)\n",
    "print(f\"pruned model sparsity sanity check {sparsity_ratio:.4f}\")\n",
    "print(\"*\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating on wikitext2\n",
      "nsamples 166\n",
      "sample 0\n",
      "loss 1.4560546875\n",
      "loss 2.009765625\n",
      "loss 2.2265625\n",
      "loss 2.041015625\n",
      "loss 1.5732421875\n",
      "loss 1.7021484375\n",
      "loss 1.4521484375\n",
      "loss 1.3447265625\n",
      "loss 1.70703125\n",
      "loss 1.8349609375\n",
      "loss 1.8837890625\n",
      "loss 1.8271484375\n",
      "loss 1.64453125\n",
      "loss 1.873046875\n",
      "loss 1.9560546875\n",
      "loss 2.041015625\n",
      "loss 1.96875\n",
      "loss 2.025390625\n",
      "loss 2.1484375\n",
      "loss 1.9384765625\n",
      "loss 1.83203125\n",
      "loss 1.5947265625\n",
      "loss 1.560546875\n",
      "loss 1.9560546875\n",
      "loss 1.978515625\n",
      "loss 1.916015625\n",
      "loss 1.96875\n",
      "loss 1.912109375\n",
      "loss 2.052734375\n",
      "loss 1.857421875\n",
      "loss 2.279296875\n",
      "loss 2.078125\n",
      "loss 2.041015625\n",
      "loss 1.8427734375\n",
      "loss 1.6396484375\n",
      "loss 1.5703125\n",
      "loss 1.501953125\n",
      "loss 1.7109375\n",
      "loss 1.7392578125\n",
      "loss 1.9580078125\n",
      "loss 1.8251953125\n",
      "loss 1.3515625\n",
      "loss 1.16796875\n",
      "loss 1.376953125\n",
      "loss 1.1728515625\n",
      "loss 1.283203125\n",
      "loss 1.47265625\n",
      "loss 1.822265625\n",
      "loss 2.275390625\n",
      "loss 2.291015625\n",
      "sample 50\n",
      "loss 2.171875\n",
      "loss 2.06640625\n",
      "loss 1.9716796875\n",
      "loss 1.8466796875\n",
      "loss 1.962890625\n",
      "loss 2.013671875\n",
      "loss 1.5751953125\n",
      "loss 2.068359375\n",
      "loss 1.7158203125\n",
      "loss 1.58203125\n",
      "loss 1.556640625\n",
      "loss 1.580078125\n",
      "loss 1.4462890625\n",
      "loss 1.5322265625\n",
      "loss 1.3515625\n",
      "loss 1.3193359375\n",
      "loss 1.5185546875\n",
      "loss 1.68359375\n",
      "loss 1.9033203125\n",
      "loss 1.982421875\n",
      "loss 2.0625\n",
      "loss 2.12109375\n",
      "loss 2.203125\n",
      "loss 1.3818359375\n",
      "loss 1.49609375\n",
      "loss 1.7587890625\n",
      "loss 1.5576171875\n",
      "loss 1.515625\n",
      "loss 1.5771484375\n",
      "loss 1.560546875\n",
      "loss 1.06640625\n",
      "loss 1.8798828125\n",
      "loss 1.8828125\n",
      "loss 2.005859375\n",
      "loss 1.951171875\n",
      "loss 1.671875\n",
      "loss 1.7919921875\n",
      "loss 1.791015625\n",
      "loss 1.8623046875\n",
      "loss 1.7666015625\n",
      "loss 1.822265625\n",
      "loss 1.8251953125\n",
      "loss 2.125\n",
      "loss 1.6796875\n",
      "loss 1.703125\n",
      "loss 1.7119140625\n",
      "loss 1.740234375\n",
      "loss 1.607421875\n",
      "loss 1.5322265625\n",
      "loss 1.498046875\n",
      "sample 100\n",
      "loss 1.818359375\n",
      "loss 1.904296875\n",
      "loss 1.8544921875\n",
      "loss 1.9931640625\n",
      "loss 2.458984375\n",
      "loss 2.34375\n",
      "loss 2.1640625\n",
      "loss 2.1171875\n",
      "loss 2.279296875\n",
      "loss 2.21484375\n",
      "loss 1.96484375\n",
      "loss 1.94140625\n",
      "loss 1.83984375\n",
      "loss 1.95703125\n",
      "loss 1.568359375\n",
      "loss 1.7578125\n",
      "loss 1.9326171875\n",
      "loss 1.8466796875\n",
      "loss 1.716796875\n",
      "loss 1.8515625\n",
      "loss 1.740234375\n",
      "loss 1.8740234375\n",
      "loss 1.8447265625\n",
      "loss 1.6494140625\n",
      "loss 1.5888671875\n",
      "loss 1.548828125\n",
      "loss 1.5234375\n",
      "loss 1.6484375\n",
      "loss 1.6650390625\n",
      "loss 1.748046875\n",
      "loss 1.9375\n",
      "loss 1.8935546875\n",
      "loss 1.9013671875\n",
      "loss 1.9775390625\n",
      "loss 1.517578125\n",
      "loss 1.814453125\n",
      "loss 1.8984375\n",
      "loss 1.9384765625\n",
      "loss 1.6630859375\n",
      "loss 1.5986328125\n",
      "loss 1.57421875\n",
      "loss 1.7236328125\n",
      "loss 1.5693359375\n",
      "loss 1.9521484375\n",
      "loss 1.654296875\n",
      "loss 1.7734375\n",
      "loss 1.6123046875\n",
      "loss 1.6494140625\n",
      "loss 1.5390625\n",
      "loss 1.966796875\n",
      "sample 150\n",
      "loss 1.669921875\n",
      "loss 1.7685546875\n",
      "loss 1.7880859375\n",
      "loss 2.056640625\n",
      "loss 1.6748046875\n",
      "loss 1.783203125\n",
      "loss 1.4921875\n",
      "loss 1.095703125\n",
      "loss 1.87109375\n",
      "loss 1.9541015625\n",
      "loss 2.138671875\n",
      "loss 1.7998046875\n",
      "loss 1.7646484375\n",
      "loss 1.9306640625\n",
      "loss 1.8466796875\n",
      "loss 2.12890625\n",
      "pruned model wikitext perplexity 5.98285436630249\n"
     ]
    }
   ],
   "source": [
    "ppl_test = eval_ppl(model, tokenizer)\n",
    "print(f\"pruned model wikitext perplexity {ppl_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load sparse model\n",
    "# model = AutoModelForCausalLM.from_pretrained(save_model, torch_dtype='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It takes a great deal of bravery to stand up and say that you are a Christian.\n"
     ]
    }
   ],
   "source": [
    "# 剪枝模型的输出结果\n",
    "output_ids = model.generate(input_ids)\n",
    "print(tokenizer.decode(output_ids[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wanda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
